# -*- coding: utf-8 -*-
"""A5_MT19133_Q1_Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v90lxQ0B7JBEE5es2i2ojMWinXg2S2jh
"""

##Cell -1 ##
###Import all library####

import pandas as pd
import numpy as np
import statistics
from sklearn import preprocessing
import math
from collections import Counter

##Cell -2 ##
#### Data Import and Test Train Split ####

#"drive/My Drive/SML_A5.csv"
data_path=input("Enter data path")
data=pd.read_csv(data_path)
data=data.drop(['No'], axis=1)
categorical=data["cbwd"].unique()
median=(statistics.median(data["pm2.5"]))
mean=(statistics.mean(data["pm2.5"]))

data["pm2.5"].fillna(value=mean,inplace = True) 
data.fillna(0,inplace = True)
print(data.shape)  ### (43824, 12)
print(data)
print("year",len(data["year"].unique()))
print("month",len(data["month"].unique()))
print("day",len(data["day"].unique()))
print("hour",len(data["hour"].unique()))
print("pm2.5",len(data["pm2.5"].unique()))
print("DWEP",len(data["DEWP"].unique()))
print("TEMP",len(data["TEMP"].unique()))
print("IWS",len(data["Iws"].unique()))
print("IS",len(data["Is"].unique()))
print("IR",len(data["Ir"].unique()))




data=data.to_numpy()


Train_Data=[]
Test_Data=[]

##Training Data 2010 & 2012
##Test Data 2011 & 2014

for item in data:
  if item[0]==2010 or item[0]==2012:
    Train_Data.append(item)
  if item[0]==2011 or item[0]==2014:
    Test_Data.append(item)
  
Train_Data=np.array(Train_Data)
Test_Data=np.array(Test_Data)



print(len(Test_Data))  ### 17520
print(len(Train_Data))  ### 17544

##Cell -3 ##
### THis code performs label encoding for the "cbwd" attribute of the data ###

print(categorical)

label_encoder = preprocessing.LabelEncoder() 

categorical1= label_encoder.fit_transform(categorical)

print(categorical1)

categorical_dict={}
i=0
for item in categorical:
  categorical_dict.update({item:categorical1[i]})
  i=i+1

print(categorical_dict)

##Cell -4 ##
## features dictionary ##

features_dict={0:"year",1:"month",2:"day",3:"hour",4:"pm2.5",5:"DEWP",6:"TEMP",7:"PRES",8:"cbwd",9:"Iws",10:"Is",11:"Ir"}

features_dict1={"year":0,"month":1,"day":2,"hour":3,"pm2.5":4,"DEWP":5,"TEMP":6,"PRES":7,"cbwd":8,"Iws":9,"Is":10,"Ir":11}

##Cell -5 ##
## Label Encoder ##
from sklearn import preprocessing 

def encoder(arr):
  label_encoder = preprocessing.LabelEncoder() 

  category_col= label_encoder.fit_transform(arr[:,8:9])

  return category_col

##Cell -6 ##
### Entropy Calculator###
def entropy_calc(arr):
  
  if len(arr)==0:
    return 0,0
  else:
    c={1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0,10:0,11:0,12:0}

    for item in arr:
      mon=item[1]
      val=c.get(mon)
      c.pop(mon)
      c.update({mon:val+1})
    
    total=0
    entropy=0
    #print(arr)
    for key in c.keys():
      val=c.get(key)
      total=total+val
  #  print("total",total)
    for key in c.keys():
      val=c.get(key)
      temp=(val/total)
  #   print("temp",temp)
      entropy=entropy+(temp*math.log(temp+0.00001,2))  

    entropy=-1*entropy

    return entropy,total

##Cell -7 ##
def permutations():
  permute=[]
  permute.append(tuple([[0],[1,2,3]]))
  permute.append(tuple([[1],[0,2,3]]))
  permute.append(tuple([[2],[0,1,3]]))
  permute.append(tuple([[3],[1,2,0]]))
  
  permute.append(tuple([[1,2],[3,0]]))
  permute.append(tuple([[1,0],[3,2]]))
  permute.append(tuple([[1,3],[0,2]]))
  
  return permute

##Cell -8 ##
#### This function returns the best feature to split the node and the respective value of the feature ####

def split_node(features,arr):
  #print("Hi3")
  print("array ki size",len(arr))
  Best_feature=features[0]
  Breaking_condition=[]
  features_range={}
  for f in features:
    if not(isinstance(arr[0][f], (int,float))):

      features_range.update({f:encoder(arr)})
    else:
      unique=[]
      for j in range(0,len(arr)):
        if arr[j][f] not in unique:
          unique.append(arr[j][f])
      features_range.update({f:unique})
  E,T=entropy_calc(arr)
  max=0
  Left=np.array([])
  Right=np.array([])

  for f in features:
    print("featurename:",features_dict.get(f))
    if not(isinstance(arr[0][f], (int,float))):
      permute=permutations()  ### Write some function ##

      for item in permute:
        L=item[0]
        R=item[1]
        arr1=[]
        arr2=[]
        for ITEM in L:
          for Item in arr:
            if Item[f] ==categorical_dict.get(ITEM):
              arr1.append(Item)

        arr1=np.array(arr1)

        for ITEM in R:
          for Item in arr:
            if Item[f] ==categorical_dict.get(ITEM):
              arr2.append(Item)

        arr2=np.array(arr2)

        if len(arr1)==0 or len(arr2)==0:
          continue
       # print("Hi4")
        E1,T1=entropy_calc(arr1)
        E2,T2=entropy_calc(arr2)
        if T1 ==0 or T2 ==0 or len(R)==0:
          continue
        else:  
          GS=E-(((T1/T)*E1)+((T2/T)*E2))
         # SI=-1*(((T1/T)*math.log(T1/T,2))+((T2/T)*math.log(T2/T,2)))
         # GR=GS/SI  
          if max<GS:
            max=GS
            Left=arr1
            Right=arr2
            Best_feature=f
            Breaking_condition=[]
            Breaking_condition.append(L)
            Breaking_condition.append(R)
      
      
    else:
      range1=features_range.get(f)
      #print(range1)
      range1.sort()
      #print(range1)
      for i in range(0,len(range1)-1):
        
        if(len(range1)/100 >1):
          L=range1[0:(i+1)*200]
          R=range1[(i+1)*200:len(range1)]
          
        
        else:
          L=range1[0:(i+1)*2]
          R=range1[(i+1)*2:len(range1)]
        arr1=[]
        arr2=[]

        for ITEM in L:
          for item in arr:
            if item[f]==ITEM:
              arr1.append(item)
        arr1=np.array(arr1)

        for ITEM in R:
          for item in arr:
            if item[f]==ITEM:
              arr2.append(item)
        arr2=np.array(arr2)
        if len(arr2)==0 or len(arr1)==0 or len(R)==0:
          continue
        #print("Hi5")
        #if len(arr1) >0:
        
        #if len(arr2) >0:
        else:
          E1,T1=entropy_calc(arr1)
          E2,T2=entropy_calc(arr2)
          GS=E-(((T1/T)*E1)+((T2/T)*E2))
          #SI=-1*(((T1/T)*math.log(T1/T,2))+((T2/T)*math.log(T2/T,2)))
          #GR=GS/SI  
          if max<GS:
            max=GS
            Left=arr1
            Right=arr2
            Best_feature=f
            Breaking_condition=[]
            Breaking_condition.append(R[0])        
    
  return Left,Right,Best_feature,Breaking_condition

#Decision_tuple=[]
##Cell -9 ##
def DT(Decision_tuple,arr,features,Level):

  
  print(arr[:1])
  if len(np.unique(arr[:,1]))==1:
    
    Decision_tuple.append(tuple([Level,"Leaf",arr[0][1]]))
    return None

  elif len(arr) <=19:
    
    mode=(Counter(arr[:,1]))
    tup=((mode.most_common()))
    tup.sort(key = lambda x: x[1],reverse=True)

    Decision_tuple.append(tuple([Level,"Leaf",tup[0][0]]))
    return None
  

  LEFT,RIGHT,F,Breaking_condition=split_node(features,arr)
  print("feature selected",F)
  print("Left node",len(LEFT))
  print("Right node",len(RIGHT))
  Decision_tuple.append(tuple([Level,features_dict.get(F),Breaking_condition]))
  
  features_updt=[]
  #for fe in features:
  #if fe != F and fe!=1:
  features_updt=features
  #print("Hi1")
  if len(LEFT)!=0:    
    DT(Decision_tuple,LEFT,features_updt,Level+1)
  #print("Hi")
  if len(RIGHT)!=0:    
  
    DT(Decision_tuple,RIGHT,features_updt,Level+1)
  return Decision_tuple

### This function changes the stucture of each Decision tree node to a tuple ###
##Cell -10 ##
def adj_ds(Decision_tuple):
  DTR=[]
  for item in Decision_tuple:
    if item[1]=='Leaf':
      temp=[]
      temp.append(item[2])
      T=tuple([item[0],item[1],temp])
      DTR.append(T)
    else:
      DTR.append(item)
  return DTR

print(DT)

##Cell -11 ##
### This function takes Decision tree as input and predict the output ###
def funct1(test,item,DT1):
  level=item[0]
  feature=item[1]
  val=item[2][0]
  item1=["","",""]

  while item1[1] != 'Leaf':

    if item[1]=='cbwd':
      
      if test[features_dict1.get(feature)] in val[0]:
        for item1 in DT1:
          if item1[0]==level+1:
            level=item1[0]
            feature=item1[1]
            print("1",item1)
            val=item1[2][0]
            break

      else:
        if test[features_dict1.get(feature)]  in val[1]:
          i=0 
          j=0
          for item1 in DT1:
            j=j+1
            if item1[0]==level+1 and i==0:
              i=i+1
            else: 
              if item1[0]==level+1 and i==1:
                DT1=DT1[j:len(DT1)]
                level=item1[0]
                feature=item1[1]
                print("2",item1)
                val=item1[2][0]
                break







    ###################################################
    else:
      if test[features_dict1.get(feature)] < val:
        for item1 in DT1:
          if item1[0]==level+1:
            level=item1[0]
            feature=item1[1]
            print("3",item1)
            val=item1[2][0]
            break

      else:
        if test[features_dict1.get(feature)]>= val:
          i=0 
          j=0
          for item1 in DT1:
            j=j+1
            if item1[0]==level+1 and i==0:
              i=i+1
            else: 
              if item1[0]==level+1 and i==1:
                DT1=DT1[j:len(DT1)]
                level=item1[0]
                feature=item1[1]
                print("4",item1)
                val=item1[2][0]
                break
  return(item1[2][0])

### This function calculate the accuracy ###
##Cell -12 ##
def accuracy(DTR):

  ct=0
  RES=[]
  for test in Test_Data:

    res=funct1(test,DTR[0],DTR)
    RES.append(res)
    act=test[1]
    if act==res:
      ct=ct+1

  acc=((ct*100)/len(Test_Data))
  return acc,RES
  #print(res,act)

##Cell -13 ##
### This function makes single decision tree ###
def make_decision_tree():
  Decision_tuple=[]
  DF=DT(Decision_tuple,Train_Data,[0,2,3,4,5,6,7,8,9,10,11],0) 
  DTR=adj_ds(DF)
  acc,RES=accuracy(DTR)
  Z=RES
  print(acc)
  return Z

##Cell -14 ##
#### Calll this function to create Decion tree , predict output, calculate MSE , MAE and STD Deviation #### 

Z=make_decision_tree()

##Cell -15 ##
#### Accuracy function for bag ####
#### Same function for random forest as well ####
def accuracy_bag(DTR):

  ct=[]
  for test in Test_Data:
    #print(DTR[0])
    res=funct1(test,DTR[0],DTR)
    act=test[1]
    ct.append(res)
  return ct

##Cell -16 ##
#### Bagging ####


def make_bag(k):
  prediction=[]
  ct=0
  Range=list(range(0,len(Train_Data)))
  print(Range)
  for j in range(0,k):
    indices=np.random.choice(Range,len(Train_Data),replace=True)
    print(indices)
    sample=[]
    for i in indices:
      sample.append(Train_Data[i])
    sample=np.array(sample)
    Decision_tuple=[]
    DD= DT(Decision_tuple,sample,[0,2,3,4,5,6,7,8,9,10,11],0) 
    DTR=adj_ds(DD)
    acc=accuracy_bag(DTR)
    prediction.append(acc)
  
  
  for k in range(0,len(Test_Data)) :
    Result=[]
    for kk in range(0,len(prediction)):
      Result.append(prediction[kk][k])
    mode=(Counter(Result))
    tup=((mode.most_common()))
    
    actual=Test_Data[k][1]

    print(actual,Result,tup)
    if actual==tup[0][0]:
      ct=ct+1
  print((ct*100)/len(Test_Data))

##Cell -17 ##
#### Calll this function to create Bag of Decion trees , predict output, calculate MSE , MAE and STD Deviation #### 


k=int(input("enter number of decision trees to make bag"))
make_bag(k)

##Cell -18 ##
#### Random forest code ####


def make_forest(k):
  prediction=[]
  ct=0
  Range=list(range(0,len(Train_Data)))
  print(Range)
  for j in range(0,k):
    indices=np.random.choice(Range,len(Train_Data),replace=True)
    print(indices)
    sample=[]
    for i in indices:
      sample.append(Train_Data[i])
    sample=np.array(sample)
    Decision_tuple=[]
    dim_num=int(math.sqrt(len([0,2,3,4,5,6,7,8,9,10,11])))
    dim=np.random.choice([0,2,3,4,5,6,7,8,9,10,11],dim_num,replace=False)
    print("dim",dim)
    DD= DT(Decision_tuple,sample,list(dim),0) 
    DTR=adj_ds(DD)
    acc=accuracy_bag(DTR)
    prediction.append(acc)
  
  
  for k in range(0,len(Test_Data)) :
    Result=[]
    for kk in range(0,len(prediction)):
      Result.append(prediction[kk][k])
    mode=(Counter(Result))
    tup=((mode.most_common()))
    
    actual=Test_Data[k][1]

    print(actual,Result,tup)
    if actual==tup[0][0]:
      ct=ct+1
  print((ct*100)/len(Test_Data))

##Cell -19 ##
#### Calll this function to create Forest of Decion trees , predict output, calculate MSE , MAE and STD Deviation #### 
k=int(input("enter number of decision trees to make bag"))
make_forest(k)